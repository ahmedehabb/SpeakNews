{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_1a63xBlswK"
      },
      "source": [
        "# This is a encoder decode Transformer used for abstractive Summarization\n",
        "## It uses subword tokenization pretrained by bert-base-cased\n",
        "## Moreover, It relies on the articles of CNN DailyMail Dataset\n",
        "## It's trained using A P100 GPU for 10000 batches\n",
        "## All hyperparameters are down "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zbxhyl_zFlWL",
        "outputId": "b8cd4889-941d-4854-c099-e97ed44642e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in c:\\users\\esc\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (4.28.1)\n",
            "Requirement already satisfied: filelock in c:\\users\\esc\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\esc\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers) (0.13.4)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\esc\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\esc\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\esc\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\esc\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers) (2023.3.23)\n",
            "Requirement already satisfied: requests in c:\\users\\esc\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers) (2.28.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\esc\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\esc\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\esc\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\esc\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: colorama in c:\\users\\esc\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\esc\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->transformers) (3.0.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\esc\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\esc\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->transformers) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\esc\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->transformers) (2022.12.7)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.1 -> 23.1.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
            "c:\\Users\\ESC\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from numpy import array\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "import torch\n",
        "!pip install transformers\n",
        "from transformers import AutoTokenizer\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import re\n",
        "import pickle\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJJIJFeblswU",
        "outputId": "692c5c81-6e1b-412f-9d0a-79a2c40ef0d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yH5cg5pSIHaZ"
      },
      "source": [
        "### Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "K_AjGkWXITKA"
      },
      "outputs": [],
      "source": [
        "news = pd.read_excel(\"../../datasets/news.xlsx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "S-rYZhayIe9x"
      },
      "outputs": [],
      "source": [
        "news.drop(['Source ', 'Time ', 'Publish Date'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "oXtxc-toIc94",
        "outputId": "c002b642-2c81-4991-a5a8-679c3c9a7ea7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Headline</th>\n",
              "      <th>Short</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4 ex-bank officials booked for cheating bank o...</td>\n",
              "      <td>The CBI on Saturday booked four former officia...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Supreme Court to go paperless in 6 months: CJI</td>\n",
              "      <td>Chief Justice JS Khehar has said the Supreme C...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>At least 3 killed, 30 injured in blast in Sylh...</td>\n",
              "      <td>At least three people were killed, including a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Why has Reliance been barred from trading in f...</td>\n",
              "      <td>Mukesh Ambani-led Reliance Industries (RIL) wa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Was stopped from entering my own studio at Tim...</td>\n",
              "      <td>TV news anchor Arnab Goswami has said he was t...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Headline  \\\n",
              "0  4 ex-bank officials booked for cheating bank o...   \n",
              "1     Supreme Court to go paperless in 6 months: CJI   \n",
              "2  At least 3 killed, 30 injured in blast in Sylh...   \n",
              "3  Why has Reliance been barred from trading in f...   \n",
              "4  Was stopped from entering my own studio at Tim...   \n",
              "\n",
              "                                               Short  \n",
              "0  The CBI on Saturday booked four former officia...  \n",
              "1  Chief Justice JS Khehar has said the Supreme C...  \n",
              "2  At least three people were killed, including a...  \n",
              "3  Mukesh Ambani-led Reliance Industries (RIL) wa...  \n",
              "4  TV news anchor Arnab Goswami has said he was t...  "
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "news.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vR2hg9themaN",
        "outputId": "31aeb31b-08e5-4b1e-af04-a8fb98264e3f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(55104, 2)"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "news.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "d4cEp3wmI2BX"
      },
      "outputs": [],
      "source": [
        "document = news['Short']\n",
        "summary = news['Headline']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2z55AhpKIdK7",
        "outputId": "fd7896fc-308f-4a2f-947d-dceeb2bb9cc5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('According to the Guinness World Records, the most generations alive in a single family have been seven.  The difference between the oldest and the youngest person in the family was about 109 years, when Augusta Bunge&#39;s great-great-great-great grandson was born on January 21, 1989. The family belonged to the United States of America.',\n",
              " 'The most generations alive in a single family have been 7')"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "document[30], summary[30]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZden_q9_eZr"
      },
      "source": [
        "#### Obtaining insights on lengths for defining maxlen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "ma4o2nGdK5Xb"
      },
      "outputs": [],
      "source": [
        "document_lengths = pd.Series([len(x) for x in document])\n",
        "summary_lengths = pd.Series([len(x) for x in summary])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXZlO99C-UXK",
        "outputId": "d8c03f18-272b-4d2f-a2f5-6104a80f99f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count    55104.000000\n",
              "mean       368.003049\n",
              "std         26.235510\n",
              "min        280.000000\n",
              "25%        350.000000\n",
              "50%        369.000000\n",
              "75%        387.000000\n",
              "max        469.000000\n",
              "dtype: float64"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "document_lengths.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALMwKMx--ZF7",
        "outputId": "8f83b119-0621-43c0-ef35-1c937a37f625"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count    55104.000000\n",
              "mean        51.620282\n",
              "std          7.267463\n",
              "min          8.000000\n",
              "25%         47.000000\n",
              "50%         51.000000\n",
              "75%         57.000000\n",
              "max         84.000000\n",
              "dtype: float64"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summary_lengths.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "cVeMilXr-bpC"
      },
      "outputs": [],
      "source": [
        "# maxlen\n",
        "# taking values > and round figured to 75th percentile\n",
        "# at the same time not leaving high variance\n",
        "encoder_maxlen = 512\n",
        "decoder_maxlen = 128"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95Zv7FIvKbTi"
      },
      "source": [
        "#### Tokenizing the texts into integer tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "7TqbpEyPMRqa"
      },
      "outputs": [],
      "source": [
        "# since < and > from default tokens cannot be removed\n",
        "filters = '!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n'\n",
        "oov_token = '[UNK]'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "TOjQOf13lswe"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"bert-base-cased\",\n",
        "    use_fast=True,\n",
        "    max_size = 1000,\n",
        "    unk_token=oov_token,\n",
        ")\n",
        "# Get the sos token\n",
        "sos_token = tokenizer.cls_token\n",
        "\n",
        "# Get the eos token\n",
        "eos_token = tokenizer.sep_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yn4uWM0uI7wp"
      },
      "source": [
        "### Preprocessing before tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbKwzBrvI1SU",
        "outputId": "a099865d-78f3-467b-cf6a-c8014e90c1ad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0    [CLS] 4 exbank officials booked for cheating b...\n",
              " 1    [CLS] Supreme Court to go paperless in 6 month...\n",
              " 2    [CLS] At least 3 killed 30 injured in blast in...\n",
              " 3    [CLS] Why has Reliance been barred from tradin...\n",
              " 4    [CLS] Was stopped from entering my own studio ...\n",
              " Name: Headline, dtype: object,\n",
              " 0    The CBI on Saturday booked four former officia...\n",
              " 1    Chief Justice JS Khehar has said the Supreme C...\n",
              " 2    At least three people were killed including a ...\n",
              " 3    Mukesh Ambaniled Reliance Industries RIL was b...\n",
              " 4    TV news anchor Arnab Goswami has said he was t...\n",
              " Name: Short, dtype: object)"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# for decoder sequence\n",
        "# remove all the filters from documents\n",
        "document = document.apply(lambda x: x.translate(str.maketrans('', '', filters)))\n",
        "# remove numbers between words which are noisy as governme39ts\n",
        "document = document.apply(lambda x: re.sub(r'(?<=\\w)\\d+(?=\\w)', '', x))\n",
        "\n",
        "\n",
        "# remove all the filters from summaries and add sos and eos tokens\n",
        "summary = summary.apply(lambda x: sos_token + ' ' + x.translate(str.maketrans('', '', filters)) + ' '+ eos_token)\n",
        "# remove numbers between words which are noisy as governme39ts\n",
        "summary = summary.apply(lambda x: re.sub(r'(?<=\\w)\\d+(?=\\w)', '', x))\n",
        "\n",
        "\n",
        "summary.head(), document.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "seEMK19N1olr"
      },
      "outputs": [],
      "source": [
        "# Train the tokenizer on the dataset\n",
        "tokenizer = tokenizer.train_new_from_iterator(document + summary, vocab_size=1000, show_progress=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tS_2XLK4ir-",
        "outputId": "7dc0ade6-0e0e-4af7-9d8a-bc56b31afcf8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "cHw2csoYImsa"
      },
      "outputs": [],
      "source": [
        "# Tokenize the strings\n",
        "# applying Padding/Truncating sequences for identical sequence lengths\n",
        "tokenized_data_inputs = [torch.tensor(tokenizer.encode(text, truncation=\"longest_first\", padding='max_length', max_length=encoder_maxlen, add_special_tokens=False)).to(device) for text in document]\n",
        "tokenized_data_outputs = [torch.tensor(tokenizer.encode(text, truncation=\"longest_first\", padding='max_length', max_length=decoder_maxlen, add_special_tokens=False)).to(device) for text in summary]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWU9Xu7OKVab",
        "outputId": "9619c6b9-7385-4298-9abd-56c5831ff003"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([414,  17, 235, 236, 395, 988, 471,  44, 695, 367, 996, 411, 598, 820,\n",
            "        207, 386,  33, 218, 380, 385, 485,  16, 614, 392,  61, 934, 807, 207,\n",
            "        411, 684, 203, 790, 411, 726, 558, 499, 429, 366, 377, 983, 209, 403,\n",
            "        413, 218, 392,  45, 212, 444, 378, 199, 232, 227, 659,  54, 464, 207,\n",
            "        382, 368, 889, 210, 448,  44, 614, 414, 620, 987, 565,  43, 217, 605,\n",
            "        367,  50, 675,  54, 204, 576, 392, 499, 367, 383, 472,  33, 218, 380,\n",
            "        385, 485,  16, 614, 395, 368,  44, 379, 381, 386, 411, 693, 392,  48,\n",
            "        462, 759, 556, 944, 206, 456, 741, 414, 879,  48, 643, 207, 587,  48,\n",
            "        449, 490, 466, 948, 770, 205, 207, 964, 408, 213, 382, 368, 586, 370,\n",
            "        492,  57, 958, 367, 443, 368, 620, 987, 703, 551, 205, 207,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0]) The CBI on Saturday booked four former officials of Syndicate Bank and six others for cheating forgery criminal conspiracy and causing ₹29 crore loss to the staterun bank The accused had availed home loans and credit from Syndicate Bank on the basis of forged and fabricated documents These funds were fraudulently transferred to the companies owned by the accused persons [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "tensor([  2,   9, 509, 228, 614, 820, 207,  44, 695, 367, 411, 684, 203, 790,\n",
            "         44, 614, 386, 199, 232, 227, 659,   3,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0]) [CLS] 4 exbank officials booked for cheating bank of ₹29 crore [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
          ]
        }
      ],
      "source": [
        "# Print the first encoded article and its summary\n",
        "print(tokenized_data_inputs[0], tokenizer.decode(tokenized_data_inputs[0]))\n",
        "print(tokenized_data_outputs[0], tokenizer.decode(tokenized_data_outputs[0]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ESm-aYR-tvx",
        "outputId": "743be5cc-9d51-435c-d35d-65b8fa803604"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "512"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(tokenized_data_inputs[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVyErXAei5_b",
        "outputId": "ee277e14-a43e-4c1c-9f4b-a7bbb26f0035"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[2, 808, 433, 43, 720, 404, 3]"
            ]
          },
          "execution_count": 135,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.encode(\"This is a test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Ryx9qx90jwXu",
        "outputId": "c2d4cf48-b0f1-412c-b959-6f2b22c34edf"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[CLS] Sing at vaim re [SEP]'"
            ]
          },
          "execution_count": 136,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode([2, 800, 450, 64, 740, 420, 3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoizyBvLKv8h",
        "outputId": "698d2d02-c061-4206-db6b-03bb34b10c1a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "execution_count": 137,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab_size = tokenizer.vocab_size\n",
        "vocab_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIP0kIIcB8Rm"
      },
      "source": [
        "### Creating dataset pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "wI-fV7eABWN6"
      },
      "outputs": [],
      "source": [
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, inputs, targets):\n",
        "        self.inputs = inputs\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.inputs[idx], self.targets[idx]\n",
        "\n",
        "batch_size = 64\n",
        "shuffle = 20000\n",
        "\n",
        "dataset = MyDataset(tokenized_data_inputs, tokenized_data_outputs)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pq4N5KSMlswi",
        "outputId": "2a6cd2b7-e6b3-4482-a7ba-5aac91a4ac3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "64 512\n",
            "64 128\n"
          ]
        }
      ],
      "source": [
        "for inputs, outputs in dataloader:\n",
        "    print(len(inputs), len(inputs[0]))\n",
        "    print(len(outputs), len(outputs[0]))\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24Pe01DMMWHc"
      },
      "source": [
        "### Masking\n",
        "\n",
        "- Padding mask for masking \"pad\" sequences\n",
        "- Lookahead mask for masking future words from contributing in prediction of current words in self attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "hN1wVQAdMVYy"
      },
      "outputs": [],
      "source": [
        "def create_padding_mask(input_sequence):\n",
        "    # input_sequence : B, T\n",
        "    mask = (input_sequence != 0).unsqueeze(-2).float()\n",
        "    return mask\n",
        "\n",
        "# # B, T, T\n",
        "# mask = create_padding_mask(inputs[0:2])\n",
        "# tx = torch.randn(2, 10, 10).to('cuda')\n",
        "# tx = tx.masked_fill(mask == 0, float('-inf'))\n",
        "# mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "UmjAPLWuMREE"
      },
      "outputs": [],
      "source": [
        "def create_look_ahead_mask(size):\n",
        "    return torch.tril(torch.ones(size, size, device=device))\n",
        "\n",
        "# create_look_ahead_mask(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8DqUBc4NFOy"
      },
      "source": [
        "### Building the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "dsjM1jZrlswk"
      },
      "outputs": [],
      "source": [
        "# tril = torch.tril(torch.ones(8, 8))\n",
        "\n",
        "# T = 3\n",
        "# wei = torch.ones(T, T) * 5\n",
        "# wei = wei.masked_fill(tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "# tril, wei"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3QBgq90lswk"
      },
      "source": [
        "#### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ut0IP2glswl",
        "outputId": "bb0c1faf-9c5d-451a-c136-5ad87aee3c24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 64 # how many independent sequences will we process in parallel?\n",
        "encoder_block_size = encoder_maxlen # what is the maximum context length for predictions?\n",
        "decoder_block_size = decoder_maxlen # what is the maximum context length for predictions?\n",
        "max_iters = 10000\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "eval_iters = 200\n",
        "n_embd = 192\n",
        "n_head = 6\n",
        "n_layer = 3\n",
        "dropout = 0.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZT-PAZ6lswl"
      },
      "source": [
        "#### Head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "t2PtQxe1lswm"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.head_size = head_size\n",
        "        \n",
        "        # TODO: move this tril to decoder block and pass it in ma attention better as a mask\n",
        "        # I'm creating this Trill variable Trill is not a parameter of the module so in sort of pytorch\n",
        "        # conventions this is called a buffer it's not a parameter and you have to call it you have to assign it to the module\n",
        "        # using a register buffer so that creates the trail, the triangle lower triangular Matrix\n",
        "        # self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        # we can also drop out here when we calculate the basically affinities and after the softmax we can drop out\n",
        "        # some of those so we can randomly prevent some of the nodes from communicating\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask, key_value = None):\n",
        "        B,T,C = x.shape\n",
        "        if key_value is None:\n",
        "            # key_value will be None if its self attention, but will be with value if it comes from other source as cross attention \n",
        "            key_value = x\n",
        "\n",
        "        k = self.key(key_value)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * self.head_size**-0.5  # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(mask[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(key_value) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rf7_a5uQOfJk"
      },
      "source": [
        "#### Multi-Headed Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "iIuFrdXnNZEC"
      },
      "outputs": [],
      "source": [
        "# class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "#     def __init__(self, d_model, num_heads):\n",
        "#         super(MultiHeadAttention, self).__init__()\n",
        "#         self.num_heads = num_heads\n",
        "#         self.d_model = d_model\n",
        "\n",
        "#         assert d_model % self.num_heads == 0\n",
        "\n",
        "#         self.depth = d_model // self.num_heads\n",
        "\n",
        "#         self.wq = tf.keras.layers.Dense(d_model)\n",
        "#         self.wk = tf.keras.layers.Dense(d_model)\n",
        "#         self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "#         self.dense = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "#     def split_heads(self, x, batch_size):\n",
        "#         # size before : Batch, SequenceLength, Embedding : (64, 400, 128) B,T,C\n",
        "#         x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "#         # size now : Batch, SequenceLength, #heads, Embedding/#heads : (64, 400, 8, 16) \n",
        "        \n",
        "#         # returned indices : Batch, #heads, SequenceLength, Embedding/#heads: (64, 8, 400, 16)\n",
        "#         return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "\n",
        "    \n",
        "#     def call(self, v, k, q, mask):\n",
        "#         batch_size = tf.shape(q)[0]\n",
        "\n",
        "#         q = self.wq(q)\n",
        "#         k = self.wk(k)\n",
        "#         v = self.wv(v)\n",
        "\n",
        "#         q = self.split_heads(q, batch_size)\n",
        "#         k = self.split_heads(k, batch_size)\n",
        "#         v = self.split_heads(v, batch_size)\n",
        "\n",
        "#         scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "#             q, k, v, mask)\n",
        "\n",
        "#         scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "#         concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
        "#         output = self.dense(concat_attention)\n",
        "            \n",
        "#         return output, attention_weights\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        # a linear transformation layer that projects the concatenated output from the self.heads module to the original embedding size n_embd.\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        \n",
        "        # a dropout layer that randomly sets some of the output values to zero during training to prevent overfitting.\n",
        "        # Dropout is something that you can add right before the residual connection back or right before the connection back into the original pathway\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask, key_value = None):\n",
        "        # mask : mask we will be applying for each head\n",
        "        # key_value : can be passed in case of cross attention.\n",
        "\n",
        "        # we run all of the heads in parallel into a list and simply concatenate all of the outputs and we're concatenating over the channel dimension\n",
        "        out = torch.cat([h(x, mask, key_value) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A49tXMVvOkOZ"
      },
      "source": [
        "### Feed Forward Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "d9-qoKuTNwKq"
      },
      "outputs": [],
      "source": [
        "# def point_wise_feed_forward_network(d_model, dff):\n",
        "#     return tf.keras.Sequential([\n",
        "#         tf.keras.layers.Dense(dff, activation='relu'),\n",
        "#         tf.keras.layers.Dense(d_model)\n",
        "#     ])\n",
        "\n",
        "\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        # The module is defined using the PyTorch nn.Sequential class, which allows us to define a sequence of layers that are applied to the input in order.\n",
        "        # a feedforward neural network module with two linear layers, a ReLU activation function, and a dropout layer. \n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            # a dropout layer that randomly sets some of the output values to zero during training to prevent overfitting.\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2RRmn2bOpW9"
      },
      "source": [
        "#### Fundamental Unit of Transformer encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "HNuoJoFWO335"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        # n_head heads each of head_size-dimensional self attention running in parallel\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "\n",
        "        # feedforward neural network purpose: before when we had the multi-headed self-attention only that did the communication, we went way too fast\n",
        "        # to calculate the logits so the tokens looked at each other but didn't really have a lot of time to think on what they found from the other tokens\n",
        "        # notice: that the feed forward here when it's applying linear this is on a per token level all the tokens do this independently so the self-attention is the communication and \n",
        "        # then once they've gathered all the data now they need to think on that data individually and so that's what feed forward is doing\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "\n",
        "        # the size of the layer Norm here is n_embd of 32. so when the layer Norm is normalizing our features it is the normalization here\n",
        "        # happens the mean and the variance are taking over 32 numbers so the batch and the time act as batch Dimensions both of\n",
        "        # them so this is kind of like a per token transformation that just normalizes the features and makes them a unit mean unit gaussian at initialization\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # this is actually something that slightly departs from the original paper you see that the [ADD and Norm] is applied after the transformation\n",
        "        # but um in now it is a bit more basically common to apply the layer Norm before the transformation so there's a reshuffling of the layer Norms \n",
        "        # so this is called the [pre-norm formulation] and that's the one that we're going to implement as well\n",
        "        x = x + self.sa(self.ln1(x), mask)\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9i6Zh8gnPqdW"
      },
      "source": [
        "#### Fundamental Unit of Transformer decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "7CVmvs6dPMRC"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        # n_head heads each of head_size-dimensional self attention running in parallel\n",
        "        self.self_attention = MultiHeadAttention(n_head, head_size)\n",
        "        \n",
        "        # cross attention\n",
        "        self.cross_attention = MultiHeadAttention(n_head, head_size)\n",
        "        \n",
        "        # feedforward neural network purpose: before when we had the multi-headed self-attention only that did the communication, we went way too fast\n",
        "        # to calculate the logits so the tokens looked at each other but didn't really have a lot of time to think on what they found from the other tokens\n",
        "        # notice: that the feed forward here when it's applying linear this is on a per token level all the tokens do this independently so the self-attention is the communication and \n",
        "        # then once they've gathered all the data now they need to think on that data individually and so that's what feed forward is doing\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "\n",
        "        # the size of the layer Norm here is n_embd of 32. so when the layer Norm is normalizing our features it is the normalization here\n",
        "        # happens the mean and the variance are taking over 32 numbers so the batch and the time act as batch Dimensions both of\n",
        "        # them so this is kind of like a per token transformation that just normalizes the features and makes them a unit mean unit gaussian at initialization\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "        self.ln3 = nn.LayerNorm(n_embd)\n",
        "\n",
        "\n",
        "    def forward(self, x, encoder_mask, decoder_mask, encoder_output):\n",
        "        # this is actually something that slightly departs from the original paper you see that the [ADD and Norm] is applied after the transformation\n",
        "        # but um in now it is a bit more basically common to apply the layer Norm before the transformation so there's a reshuffling of the layer Norms \n",
        "        # so this is called the [pre-norm formulation] and that's the one that we're going to implement as well\n",
        "        x = x + self.self_attention(self.ln1(x), decoder_mask)\n",
        "        x = x + self.cross_attention(self.ln2(x), encoder_mask, encoder_output)\n",
        "        x = x + self.ffwd(self.ln3(x))\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zt5MUc_QNid"
      },
      "source": [
        "#### Encoder consisting of multiple EncoderLayer(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "BrbnTwijQJ-h"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        # take care now embedding size (= n_embd) != vocab size \n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        # positional encoding lookup \n",
        "        self.position_embedding_table = nn.Embedding(encoder_block_size, n_embd)\n",
        "\n",
        "        self.blocks = nn.Sequential(*[EncoderLayer(n_embd, n_head) for _ in range(n_layer)])\n",
        "        \n",
        "\n",
        "    def forward(self, idx, encoder_mask):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx is both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        \n",
        "        # now total embedding = token embedding + positional embedding\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        \n",
        "        # pass x into\n",
        "        for i in range(n_layer):\n",
        "            x = self.blocks[i](x, encoder_mask) # (B,T,C)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4N5LrNrvRexg"
      },
      "source": [
        "#### Decoder consisting of multiple DecoderLayer(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "UmeqkZrIRbSB"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        # take care now embedding size (= n_embd) != vocab size \n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        # positional encoding lookup \n",
        "        self.position_embedding_table = nn.Embedding(decoder_block_size, n_embd)\n",
        "\n",
        "        self.blocks = nn.Sequential(*[DecoderLayer(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "\n",
        "    def forward(self, idx, encoder_mask, decoder_mask, encoder_output):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx is both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        \n",
        "        # now total embedding = token embedding + positional embedding\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        \n",
        "        # pass x into\n",
        "        # pass x into\n",
        "        for i in range(n_layer):\n",
        "          x = self.blocks[i](x, encoder_mask, decoder_mask, encoder_output) # (B,T,C)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbMNK_bzSHnh"
      },
      "source": [
        "#### Finally, the Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "FXHRG-o4R9Mc"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = Encoder()\n",
        "        self.decoder = Decoder()\n",
        "        # final layer norm at the end of the transfomer\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        \n",
        "        # a fully connected (linear) layer by performing a linear transformation on the input tensor\n",
        "        # with a weight matrix of size (n_embd, vocab_size) and adding a bias vector of size (vocab_size,)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, encoder_input_idx, encoding_mask, decoder_input_idx, decoder_target_idx, decoder_mask):\n",
        "        \n",
        "        encoder_output = self.encoder(encoder_input_idx, encoding_mask)\n",
        "\n",
        "        decoder_output = self.decoder(decoder_input_idx, encoding_mask, decoder_mask, encoder_output)\n",
        "\n",
        "\n",
        "        # pass x into\n",
        "        decoder_output = self.ln_f(decoder_output) # (B,T,C)\n",
        "\n",
        "\n",
        "        # logits is the ouput of the fully connected (linear) layer now given input decoder_output\n",
        "        logits = self.lm_head(decoder_output) # (B,T,vocab_size)\n",
        "        \n",
        "        if decoder_target_idx is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            decoder_target_idx = decoder_target_idx.reshape(B*T)\n",
        "            mask = (decoder_target_idx != 0) # create a mask of non-padding tokens\n",
        "            loss = F.cross_entropy(logits[mask], decoder_target_idx[mask])\n",
        "\n",
        "        return logits, loss\n",
        "    \n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "\n",
        "            # because now we're using positional embeddings we can never have more than block size coming in because if idx is\n",
        "            # more than block size then our position embedding table is going to run out of scope because it only has embeddings for up to block size \n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -decoder_block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOGvkYDNTjIj"
      },
      "source": [
        "#### Adam optimizer with custom learning rate scheduling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "tfiynCLlTL8C"
      },
      "outputs": [],
      "source": [
        "# class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "#     def __init__(self, d_model, warmup_steps=4000):\n",
        "#         super(CustomSchedule, self).__init__()\n",
        "\n",
        "#         self.d_model = d_model\n",
        "#         self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "#         self.warmup_steps = warmup_steps\n",
        "    \n",
        "#     def __call__(self, step):\n",
        "#         arg1 = tf.math.rsqrt(step)\n",
        "#         arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "#         return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "5Ia0pqLUlsws"
      },
      "outputs": [],
      "source": [
        "model = Transformer()\n",
        "m = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f56BGiVXU_Dk"
      },
      "source": [
        "#### Masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "FZxHuyZxU5Pa"
      },
      "outputs": [],
      "source": [
        "def create_masks(input, target):\n",
        "    encoder_mask = create_padding_mask(input)\n",
        "    look_ahead_mask = create_look_ahead_mask(target.shape[1])\n",
        "    dec_target_padding_mask = create_padding_mask(target)\n",
        "    decoder_mask = torch.minimum(dec_target_padding_mask, look_ahead_mask)\n",
        "  \n",
        "    return encoder_mask, decoder_mask\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsVdrENTUERY"
      },
      "source": [
        "#### Defining losses and other metrics "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msSFDBBIlswt",
        "outputId": "09bf8fba-5fa4-4aea-d561-1bc318a499f7"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'dataloader' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32me:\\8th Semester CCE\\GP\\Natural Language Processing\\Transfomers\\encoder decoder transformers\\Abstractive Summarizer.ipynb Cell 61\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/8th%20Semester%20CCE/GP/Natural%20Language%20Processing/Transfomers/encoder%20decoder%20transformers/Abstractive%20Summarizer.ipynb#Y114sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m inputs, outputs \u001b[39min\u001b[39;00m dataloader:\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/8th%20Semester%20CCE/GP/Natural%20Language%20Processing/Transfomers/encoder%20decoder%20transformers/Abstractive%20Summarizer.ipynb#Y114sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(inputs), \u001b[39mlen\u001b[39m(inputs[\u001b[39m0\u001b[39m]))\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/8th%20Semester%20CCE/GP/Natural%20Language%20Processing/Transfomers/encoder%20decoder%20transformers/Abstractive%20Summarizer.ipynb#Y114sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(outputs), \u001b[39mlen\u001b[39m(outputs[\u001b[39m0\u001b[39m]))\n",
            "\u001b[1;31mNameError\u001b[0m: name 'dataloader' is not defined"
          ]
        }
      ],
      "source": [
        "for inputs, outputs in dataloader:\n",
        "    print(len(inputs), len(inputs[0]))\n",
        "    print(len(outputs), len(outputs[0]))\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "id": "yWZQgk-9lswu"
      },
      "outputs": [],
      "source": [
        "# data loading\n",
        "def get_batch():#split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    # data = train_data if split == 'train' else val_data\n",
        "    batch = random.choice(list(dataloader))\n",
        "    x, y = batch\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "# @torch.no_grad() this line says to pytorch to prevent backprop since we will be evaluating not real training\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    # let model be in evaluation phase so layers like normalization, .. change their behaviour at inference time\n",
        "    model.eval()\n",
        "    for split in ['train']: #, 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch() #split)\n",
        "            target_input = Y[:, :-1]\n",
        "            target_real = Y[:, 1:]\n",
        "            encoder_mask, decoder_mask = create_masks(X, target_input)\n",
        "            logits, loss = model(X, encoder_mask, target_input, target_real, decoder_mask)\n",
        "                                \n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    # back to training phase\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "  \n",
        "    \n",
        "    \n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ip1-943kTXXK",
        "outputId": "ee47c997-ea73-432f-b043-0fc8bb3bacaf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3.810088 M parameters\n"
          ]
        }
      ],
      "source": [
        "# learning_rate = CustomSchedule(float(d_model))\n",
        "# optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "id": "uW4LA_45T4Aa"
      },
      "outputs": [],
      "source": [
        "# loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "# def loss_function(real, pred):\n",
        "#     mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "#     loss_ = loss_object(real, pred)\n",
        "\n",
        "#     mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "#     loss_ *= mask\n",
        "\n",
        "#     return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "id": "Ze0u6xxXT7dI"
      },
      "outputs": [],
      "source": [
        "# train_loss = tf.keras.metrics.Mean(name='train_loss')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XvKy3v6ULnO"
      },
      "source": [
        "#### Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfpI0gS4c06c"
      },
      "source": [
        "#### Training steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "id": "xmVOMzkrczgl"
      },
      "outputs": [],
      "source": [
        "def train_step(input, target):\n",
        "    target_input = target[:, :-1]\n",
        "    target_real = target[:, 1:]\n",
        "\n",
        "    encoder_mask, decoder_mask = create_masks(input, target_input)\n",
        "    \n",
        "    \n",
        "    logits, loss = model(\n",
        "        input,\n",
        "        encoder_mask,\n",
        "        target_input,\n",
        "        target_real,\n",
        "        decoder_mask,\n",
        "    )\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xORKpv69dSW5",
        "outputId": "6bfc85f6-b2b3-412f-c435-69f7459e071e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: train loss 7.0860\n",
            "step 500: train loss 3.6643\n",
            "Time taken for 1 epoch: 505.03278613090515 secs\n",
            "\n",
            "step 0: train loss 3.1695\n",
            "step 500: train loss 2.7930\n",
            "Time taken for 1 epoch: 492.1347658634186 secs\n",
            "\n",
            "step 0: train loss 2.6130\n",
            "step 500: train loss 2.3931\n",
            "Time taken for 1 epoch: 515.6874475479126 secs\n",
            "\n",
            "step 0: train loss 2.1328\n",
            "step 500: train loss 1.7971\n",
            "Time taken for 1 epoch: 517.3843352794647 secs\n",
            "\n",
            "step 0: train loss 1.6653\n",
            "step 500: train loss 1.5609\n",
            "Time taken for 1 epoch: 493.74125814437866 secs\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(5):\n",
        "    start = time.time()\n",
        "\n",
        "    # train_loss.reset_states()\n",
        "    for (batch, (inputs, outputs)) in enumerate(dataloader):\n",
        "        \n",
        "        # (inputs, outputs) here represent batch of examples\n",
        "        # inputs: Batch_size * input_sequence_length\n",
        "        # outputs: Batch_size * output_sequence_length\n",
        "        \n",
        "        # every once in a while evaluate the loss on train and val sets\n",
        "        if batch % eval_interval == 0 or batch == max_iters - 1:\n",
        "            losses = estimate_loss()\n",
        "            print(f\"step {batch}: train loss {losses['train']:.4f}\") #, val loss {losses['val']:.4f}\")\n",
        "        \n",
        "        \n",
        "        train_step(inputs, outputs)\n",
        "      \n",
        "    # if (epoch + 1) % 5 == 0:\n",
        "    #     ckpt_save_path = ckpt_manager.save()\n",
        "    #     print ('Saving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))\n",
        "    \n",
        "    # print ('Epoch {} Loss {:.4f}'.format(epoch + 1, train_loss.result()))\n",
        "\n",
        "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Obnph7PkRlmD"
      },
      "source": [
        "### Checkpointing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vZp8TiJhRpJM"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'epoch' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32me:\\8th Semester CCE\\GP\\Natural Language Processing\\Transfomers\\encoder decoder transformers\\Abstractive Summarizer.ipynb Cell 71\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/8th%20Semester%20CCE/GP/Natural%20Language%20Processing/Transfomers/encoder%20decoder%20transformers/Abstractive%20Summarizer.ipynb#Y133sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m   torch\u001b[39m.\u001b[39msave(checkpoint, checkpoint_path)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/8th%20Semester%20CCE/GP/Natural%20Language%20Processing/Transfomers/encoder%20decoder%20transformers/Abstractive%20Summarizer.ipynb#Y133sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# save model, optimizer, losses['train']\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/8th%20Semester%20CCE/GP/Natural%20Language%20Processing/Transfomers/encoder%20decoder%20transformers/Abstractive%20Summarizer.ipynb#Y133sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m save(epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, model, optimizer, losses[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m])\n",
            "\u001b[1;31mNameError\u001b[0m: name 'epoch' is not defined"
          ]
        }
      ],
      "source": [
        "checkpoint_path = \"./checkpoints/model.pt\"\n",
        "\n",
        "def save(epoch, model, optimizer, loss):\n",
        "  checkpoint = {\n",
        "      'epoch': epoch,\n",
        "      'model_state_dict': model.state_dict(),\n",
        "      'optimizer_state_dict': optimizer.state_dict(),\n",
        "      'loss': loss,\n",
        "  }\n",
        "  torch.save(checkpoint, checkpoint_path)\n",
        "\n",
        "# save model, optimizer, losses['train']\n",
        "save(epoch + 1, model, optimizer, losses['train'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVbEUCZagJ0G"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMbqGTixu1cl"
      },
      "source": [
        "#### Predicting one word at a time at the decoder and appending it to the output; then taking the complete sequence as an input to the decoder and repeating until maxlen or stop keyword appears"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "id": "TGnDibqx8JyZ"
      },
      "outputs": [],
      "source": [
        "decoder_input = tokenizer.encode('[CLS]', add_special_tokens=False)  # will be list of 1 \n",
        "# convert it to a tensor and add new dimension to be like batchsize of 1 and sequence\n",
        "output = torch.tensor(decoder_input, device=device).unsqueeze(0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "F5D5cv2Jd8-6"
      },
      "outputs": [],
      "source": [
        "# turn of grad while evaluaring model\n",
        "@torch.no_grad()\n",
        "def evaluate(input_document):\n",
        "  \n",
        "    # let model be in evaluation phase so layers like normalization, .. change their behaviour at inference time\n",
        "    model.eval()\n",
        "    \n",
        "    # TODO:: must add add_special_tokens=False later after this session\n",
        "    input_document = torch.tensor(tokenizer.encode(input_document, truncation=\"longest_first\", padding='max_length', max_length=encoder_maxlen), device=device)\n",
        "    \n",
        "    # make it of size (1, input_document_size) to match transformers convention of (batch, input_documents_size)\n",
        "    encoder_input = input_document.unsqueeze(0)\n",
        "\n",
        "    \n",
        "    decoder_input = tokenizer.encode(\"[CLS]\", add_special_tokens=False)  # will be list of 1 \n",
        "    \n",
        "    # make it of size (1, input_document_size) to match transformers convention of (batch, input_documents_size)\n",
        "    output = torch.tensor(decoder_input, device=device).unsqueeze(0)\n",
        "    \n",
        "    for i in range(decoder_maxlen):\n",
        "        encoder_mask, decoder_mask = create_masks(encoder_input, output)\n",
        "        \n",
        "        logits, _ = model(encoder_input, encoder_mask, output, None, decoder_mask)\n",
        "        # in this case : logits size (B,T,vocab_size)\n",
        "        \n",
        "        # predictions, attention_weights = transformer(\n",
        "        #     encoder_input, \n",
        "        #     output,\n",
        "        #     False,\n",
        "        #     enc_padding_mask,\n",
        "        #     combined_mask,\n",
        "        #     dec_padding_mask\n",
        "        # )\n",
        "\n",
        "        logits = logits[: ,-1:, :]\n",
        "        predicted_id = torch.argmax(logits, dim=-1)\n",
        "\n",
        "        if predicted_id == tokenizer.sep_token_id:\n",
        "            return output\n",
        "\n",
        "        output = torch.cat((output, predicted_id), dim=-1)\n",
        "\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "UkpdiW6wnmiS"
      },
      "outputs": [],
      "source": [
        "def summarize(input_document):\n",
        "    # not considering attention weights for now, can be used to plot attention heatmaps in the future\n",
        "    summarized = evaluate(input_document=input_document)\n",
        "    return tokenizer.decode(summarized.view(-1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNVOWPXFIn0k",
        "outputId": "46661e5a-9c88-46ed-9874-1acfa6994a1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "index:  10977.0\n",
            "Document: \n",
            " The UK government is set to construct a tunnel underneath Stonehenge in an effort to reduce traffic congestion Transport Secretary Chris Grayling said the 2kilometre tunnel could improve the environment around the 5yearold World Heritage Site and help the local economy by linking people with jobs Meanwhile the decision has been criticised by environmentalists and historians\n",
            "Real Summary: \n",
            " [CLS] UK to build tunnel underneath Stonehenge [SEP]\n",
            "Model Summary: \n",
            " [CLS] UK govt to constructure underneath Stonehenge\n",
            "--------------------------------------\n",
            "\n",
            "index:  52456.0\n",
            "Document: \n",
            " Turkmenistan President Kurbanguly Berdymukhamedov has drafted a new Constitution to remove the upper age limit for Presidential candidates and extend the Presidential term to seven years from the current fiveyearterm With no limit on the number of terms a President can serve under the existing Constitution the new Constitution will potentially allow President Berdymukhamedov to rule for life\n",
            "Real Summary: \n",
            " [CLS] Turkmenistan Prez rewrites presidential rules [SEP]\n",
            "Model Summary: \n",
            " [CLS] Turkmenistan Prez drafted for removed for President\n",
            "--------------------------------------\n",
            "\n",
            "index:  20172.0\n",
            "Document: \n",
            " Former West Indies cricketer Brian Lara feels that no international cricket team is afraid of the Australian cricket team anymore The West Indies legend also claimed that Australia has lost its lustre and is no longer a powerful force they used to be a decade back According to Lara South Africa can possibly beat any team in the world\n",
            "Real Summary: \n",
            " [CLS] Nobodys scared of the Australian team anymore Brian Lara [SEP]\n",
            "Model Summary: \n",
            " [CLS] Australia cricket team has been lose 13 Australia\n",
            "--------------------------------------\n",
            "\n",
            "index:  28254.0\n",
            "Document: \n",
            " The Telangana government on Wednesday signed an agreement with the Indian Space Research Organisation ISRO to boost the content of Mana TV an initiative started in undivided Andhra Pradesh for the benefit of the viewers in the state Mana TV was started to provide programmes on distance education telemedicine and agriculture extension This agreement would also provide coaching to students\n",
            "Real Summary: \n",
            " [CLS] Telangana ISRO ink pact over improving TV content [SEP]\n",
            "Model Summary: \n",
            " [CLS] Telangana signs ISRO to boost TV\n",
            "--------------------------------------\n",
            "\n",
            "index:  2650.0\n",
            "Document: \n",
            " Ravichandran Ashwin mocked Australias Mitchell Starc after claiming his wicket during the Bengaluru Test on Tuesday by imitating the latters gesture of pointing to the forehead Starc had earlier gestured similarly towards Indian opener Abhinav Mukund when the latter had hit his bouncer for a six on day three Ashwin repeated the same action when he bowled Starc\n",
            "Real Summary: \n",
            " [CLS] Ashwin mocks Starc imitates his 3hitting forehead9 gesture [SEP]\n",
            "Model Summary: \n",
            " [CLS] Ravichandran Ashwin mocks Australia Mitchell Starc\n",
            "--------------------------------------\n",
            "\n",
            "index:  41964.0\n",
            "Document: \n",
            " Sarbananda Sonowal took oath as the Chief Minister of Assam on Tuesday becoming the first ever BJP candidate to hold the post in the state His swearingin ceremony was also attended by Prime Minister Narendra Modi and other Union Ministers Winning 86 seats in the 1member Assembly the BJP and its allies have ended the Congress’ 1yearreign in the state\n",
            "Real Summary: \n",
            " [CLS] Sonowal becomes BJPs first CM in Assam [SEP]\n",
            "Model Summary: \n",
            " [CLS] Assam oath as Assam took oath as Assam\n",
            "--------------------------------------\n",
            "\n",
            "index:  26787.0\n",
            "Document: \n",
            " The BJP on Saturday organised a 3protest fire9 in all districts across Kerala to protest against terror outfit Islamic State and the alleged government laxity in curbing terrorlinked activities in the state Further BJP leader and MLA O Rajagopal said the CPIMled LDF Kerala government was trying to portray the fight against terrorism as one against a particular religion\n",
            "Real Summary: \n",
            " [CLS] BJP organises 3protest fire9 over terrorism in Kerala [SEP]\n",
            "Model Summary: \n",
            " [CLS] BJP organised 3protest fire9 in all districts\n",
            "--------------------------------------\n",
            "\n",
            "index:  34977.0\n",
            "Document: \n",
            " The Madras High Court has directed the Tamil Nadu government to pay compensation to the parents of 47 missing children within four months A state government report stated there were 57 child missing cases pending investigation and compensation had been paid in 10 of them The court earlier directed the government to form a special childmissing squad for such cases\n",
            "Real Summary: \n",
            " [CLS] TN govt to pay compensation in child missing cases [SEP]\n",
            "Model Summary: \n",
            " [CLS] Tamil Nadu govt to pay compensation to pay ₹70\n",
            "--------------------------------------\n",
            "\n",
            "index:  43776.0\n",
            "Document: \n",
            " After German MPs voted in favour of calling the killing of Armenians by Ottoman forces in 15 a genocide Turkey on Thursday recalled its Ambassador from Germany Turkey had earlier warned that if the incident was termed a genocide it would test relations between the two countries In 15 Armenians and Christian minorities in the Ottoman empire were expelled \n",
            "Real Summary: \n",
            " [CLS] Turkey recalls its Ambassador from Germany [SEP]\n",
            "Model Summary: \n",
            " [CLS] Turkey makes genocide Turkey on Armenians\n",
            "--------------------------------------\n",
            "\n",
            "index:  10361.0\n",
            "Document: \n",
            " World number 24 German tennis player Alexander Zverev broke his racquet after smashing it into the court twice during his first round match against Netherlands9 Robin Haase in the Australian Open Zverev frustratedly hit the racquet into the court after losing the first game of the fourth set Zverev was subsequently warned by the chair umpire for his conduct \n",
            "Real Summary: \n",
            " [CLS] Player breaks racquet after losing a game in Australian Open [SEP]\n",
            "Model Summary: \n",
            " [CLS] Zerman tennis player Zer Zer Zer Zerman tennis player\n",
            "--------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "  # generate a random number between low and high\n",
        "  index = random.uniform(0, len(document))//1\n",
        "  print(\"index: \",index)\n",
        "  print(\"Document: \\n\",document[index])\n",
        "  print(\"Real Summary: \\n\", summary[index])\n",
        "  print(\"Model Summary: \\n\", summarize(document[index]))\n",
        "  print(\"--------------------------------------\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Loading model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "gNpFjTgdGZPT"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = \"./checkpoints/model.pt\"\n",
        "\n",
        "def load():\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  model = Transformer().to(device)  # create an instance of your model\n",
        "  optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "  \n",
        "  checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "  epoch = checkpoint['epoch']\n",
        "  loss = checkpoint['loss']\n",
        "\n",
        "  model.to(device)\n",
        "  return model, optimizer, epoch, loss\n",
        "  \n",
        "# test load\n",
        "model, optimizer, epoch, loss = load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6oSyIpmSYfz-",
        "outputId": "dcbaed8f-6247-46cf-baba-3373d92a408d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5609) 4\n"
          ]
        }
      ],
      "source": [
        "print(loss, epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "83o56VqqYzPn"
      },
      "outputs": [],
      "source": [
        "vocab_size=1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'[CLS] WPv does the wednfing head of hickey in riots'"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summarize(\"brazilian police on wednesday arrested the head of the european olympic committees patrick hickey in rio de janeiro over illegal sales of olympic tickets police said hickey and at least six others are accused of illegally passing on tickets for the games to be sold on at extortionate prices hickey was taken to hospital after his arrest\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "fafa48adf760e89863c852971c7f6e563e041a06f543659c1b2ef78dcf8d05d3"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
